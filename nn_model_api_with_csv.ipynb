{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: \n",
    "# Example call:   tf_variable = get_nn_parameter(...)\n",
    "def get_nn_parameter(variable_scope, variable_name, dim1, dim2):\n",
    "    '''\n",
    "    Used to retrieve or create new NN parameters (weights & biases)\n",
    "    When calling, the corresponding NNparameter's dimensions need to be specified too.\n",
    "    Returns a tensorflow variable. Note that NN parameters need to be tensorflow variables\n",
    "    so that values can be changed whenever needed when training. Also note that it is \n",
    "    explicitly defined that the created variable is TRAINABLE.\n",
    "    '''\n",
    "    with tf.variable_scope(variable_scope, reuse=tf.AUTO_REUSE):\n",
    "      v = tf.get_variable(variable_name, \n",
    "                          [dim1, dim2], \n",
    "                          trainable=True, \n",
    "                          initializer = tf.contrib.layers.xavier_initializer())\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: \n",
    "# Example call:   nn_parameters = initialize_layer_parameters([3, 5, 5, 2])\n",
    "def initialize_layer_parameters(num_units_in_layers):\n",
    "    '''\n",
    "    NOTE THAT THE LAST LAYER HAS TO HAVE AT LEAST 2 UNITS BCZ SOFTMAX IS USED IN THIS NN MODEL\n",
    "    Returns a dictionary of created weights and biases for all layers of the NN.\n",
    "    Note that # units can vary in each layer.\n",
    "    Exmaple return: parameters = {\"W1\": tf_variable_for_W1, \"b1\": tf_variable_for_b1, ...}\n",
    "    '''\n",
    "    \n",
    "    parameters = {}\n",
    "    L = len(num_units_in_layers)\n",
    "     \n",
    "    for i in range (1, L):\n",
    "        #print(\"W\" + str(i) + \" \" + str(num_units_in_layers[i]) + \" \" + str(num_units_in_layers[i-1]))\n",
    "        temp_weight = get_nn_parameter(\"weights\",\n",
    "                                       \"W\"+str(i), \n",
    "                                       num_units_in_layers[i], \n",
    "                                       num_units_in_layers[i-1])\n",
    "        parameters.update({\"W\" + str(i) : temp_weight})  \n",
    "        \n",
    "        #print(\"b\" + str(i) + \" \" + str(num_units_in_layers[i]) + \" \" + str(1))\n",
    "        temp_bias = get_nn_parameter(\"biases\",\n",
    "                                     \"b\"+str(i), \n",
    "                                     num_units_in_layers[i], \n",
    "                                     1)\n",
    "        parameters.update({\"b\" + str(i) : temp_bias})  \n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: \n",
    "# Example call: ZL = forward_propagation_with_relu(X_train, [3, 5, 5, 2], nn_parameters)\n",
    "def forward_propagation_with_relu(X, num_units_in_layers, parameters):\n",
    "    '''\n",
    "    Returns ZL, which is the linear output of the output layer (layer L)\n",
    "    AL is also calculated but it is not returned or used. The optimizer function takes ZL as input, not the AL.\n",
    "    What activation function is used in output layer plays role when calculating the cost \n",
    "    so that you need to call the right cost (or cross entropy) function in compute_cost(...) function\n",
    "    '''\n",
    "    L = len(num_units_in_layers)\n",
    "    \n",
    "    A_temp = tf.transpose(X)\n",
    "    for i in range (1, L):\n",
    "        #W = get_nn_parameter(\"weights\", \"W\"+str(i), num_units_in_layers[i], num_units_in_layers[i-1])\n",
    "        W = parameters.get(\"W\"+str(i))\n",
    "        #b = get_nn_parameter(\"biases\", \"b\"+str(i), num_units_in_layers[i], 1)\n",
    "        b = parameters.get(\"b\"+str(i))\n",
    "        Z_temp = tf.add(tf.matmul(W, A_temp), b)\n",
    "        A_temp = tf.nn.relu(Z_temp)       # Example: A1 = relu(Z1)1, Note that A in the last (output) \n",
    "                                          # layer is irrelevant bcz we return ZL\n",
    "\n",
    "    return Z_temp   #This is the linear output of last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION\n",
    "# This function requires update if softmax is not used in the output layer\n",
    "def compute_cost(ZL, Y, parameters, lambd, mb_size):\n",
    "    \"\"\"\n",
    "    This function should be used for multinomial mutually exclusive classification, i.e. pick one out of N classes. \n",
    "    Also applicable when N = 2.\n",
    "    The labels must be one-hot encoded or can contain soft class probabilities: a particular example can belong to\n",
    "    class A with 50% probability and class B with 50% probability. Note that strictly speaking it doesn't mean that\n",
    "    it belongs to both classes, but one can interpret the probabilities this way.\n",
    "    \n",
    "    Arguments:\n",
    "    ZL -- output of forward propagation (output of the last LINEAR unit)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as ZL\n",
    "    mb_size -- number of samples in the given mini-batch\n",
    "    lambd -- lambda regularization parameter (regularization deactivated is lambd=0.)\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(ZL)\n",
    "    labels = Y\n",
    "    \n",
    "    # This cost calculation is unregularized. cost = (1/m) sum(Loss(y_hat(i), y(i))), where i = 1,..,mb_size \n",
    "    #tf.reduce_mean(..) function finds the mean of costs of examples in the given mini-batch\n",
    "    cost_unregularized = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = labels))\n",
    "    \n",
    "    # Add L2 regularization: cost += (lambd / (2 * mb_size)) * sum(W(i,j)**2), where i:1,..,n[l] and j:1,..,n[l-1] \n",
    "    # L:number of layers. Since the dict parameters includes both W and b, it needs to be divided with 2 to find L\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    # The list will have L elements, each holding the sum of weight matrix values in each layer. Later, these\n",
    "    # weight values need to be summed up again\n",
    "    list_sum_weights = []\n",
    "    \n",
    "    for i in range (0, L):\n",
    "        list_sum_weights.append(tf.nn.l2_loss(parameters.get(\"W\"+str(i+1))))\n",
    "    \n",
    "    # in the following calculation, since the l2_loss returns \"sum(t ** 2) / 2\", where the sum of squares is already\n",
    "    # divided by 2, there is no need to bultiply the mb_size with 2\n",
    "    #regularization_effect = (lambd / mb_size) * sum(list_sum_weights)\n",
    "    regularization_effect = tf.multiply((lambd / mb_size), tf.add_n(list_sum_weights))\n",
    "    cost = tf.add(cost_unregularized, regularization_effect)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that 4 statements below need to updated prior to any use in any project because number of columns \n",
    "# (# features and labels) may change in each project\n",
    "\n",
    "# This function only uses tf.TextLineReader(...) and does not read other types of files i.e. binary files\n",
    "def get_next_mini_batch(mini_batch_size, num_epochs, input_paths, shuffle):\n",
    "\n",
    "    # string_input_producer creates a FIFO queue for holding the filenames until the reader needs them\n",
    "    filename_queue = tf.train.string_input_producer(input_paths,\n",
    "                                                    num_epochs,\n",
    "                                                    shuffle)\n",
    "\n",
    "    # Select the type of reader that will be used to read the CSV files down below.\n",
    "    reader = tf.TextLineReader(skip_header_lines=1)\n",
    "\n",
    "    # reader.read(..) just reads 1 row at a time\n",
    "    key, value = reader.read(filename_queue)\n",
    "\n",
    "    # UPDATE-1 IN EACH PROJECT (depending on default values for each column)\n",
    "    # Determine default values for each column in case data is missing\n",
    "    record_defaults = [[\"\"], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]\n",
    "\n",
    "    # UPDATE-2 IN EACH PROJECT (depending on #columns)\n",
    "    # The decode_csv op parses the result of reader.read_up_to(...) into a list of tensors.\n",
    "    # For instance, col2 below is a list of tensors.\n",
    "    col1, col2, col3, col4, col5, col6, col7, col8 = tf.decode_csv(value, record_defaults)\n",
    "\n",
    "    # UPDATE-3-4 (FINAL) IN EACH PROJECT (depending on # features & labels)\n",
    "    # define which columns constitute features and which columns are labels and stack them together\n",
    "    features = tf.stack([col2, col3, col4, col5, col6])\n",
    "    labels = tf.stack([col7, col8])\n",
    "\n",
    "\n",
    "    min_after_dequeue = mini_batch_size * 3\n",
    "    capacity = min_after_dequeue + 10 * mini_batch_size\n",
    "\n",
    "    X_mini_batch, Y_mini_batch = tf.train.shuffle_batch([features, labels], \n",
    "                                                        batch_size=mini_batch_size, \n",
    "                                                        capacity=capacity,\n",
    "                                                        min_after_dequeue = min_after_dequeue,\n",
    "                                                        allow_smaller_final_batch = True)\n",
    "    \n",
    "    return X_mini_batch, Y_mini_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(train_input_paths, X_test, Y_test, learning_rate, num_epochs,\n",
    "             minibatch_size, num_units_in_layers, lambd, print_cost):\n",
    "    \"\"\"\n",
    "    Returns NN parameters after the completion of training.\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.reset_default_graph()     # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)       # tf.reset_default_graph() needs to be run first before calling tf.set_random_seed(..)\n",
    "    \n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # To be used to get the next mini batch during the training\n",
    "    X_mini_batch, Y_mini_batch = get_next_mini_batch(minibatch_size, \n",
    "                                                     num_epochs, \n",
    "                                                     train_input_paths, \n",
    "                                                     True)\n",
    "    \n",
    "    # To be used to get the next mini batch when calculating/evaluating accuracy\n",
    "    X_eval_batch, Y_eval_batch = get_next_mini_batch(minibatch_size, \n",
    "                                                     1,                     # Only run through dataset once\n",
    "                                                     train_input_paths, \n",
    "                                                     False)                 # Do NOT shuffle when calculating accuracy\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_layer_parameters(num_units_in_layers)\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    ZL = forward_propagation_with_relu(X_mini_batch, num_units_in_layers, parameters)\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    cost = compute_cost(ZL, Y_mini_batch, parameters, lambd, \n",
    "                        mb_size=tf.cast(X_mini_batch.shape[1], dtype=tf.float32))\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    optimizer =  tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init_global_var = tf.global_variables_initializer() \n",
    "    init_local_var = tf.local_variables_initializer()   \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "      sess.run(init_global_var)\n",
    "      # initializing local variables needed to be able to set num_epochs\n",
    "      sess.run(init_local_var)\n",
    "\n",
    "      # Start populating the filename queue.\n",
    "      # tf.train.start_queue_runners(...) needs to be called before populating the queue ...\n",
    "      # before you call run or eval to execute the read\n",
    "      coord = tf.train.Coordinator()\n",
    "      threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "      cost_of_model = 0\n",
    "      nr_of_minibatches = 0\n",
    "      try:\n",
    "        while not coord.should_stop():\n",
    "          _ , minibatch_cost = sess.run([optimizer, cost])\n",
    "            \n",
    "          nr_of_minibatches += 1\n",
    "          cost_of_model += minibatch_cost\n",
    "          cost_of_model /= nr_of_minibatches\n",
    "        \n",
    "          # Print the cost every epoch\n",
    "          if print_cost == True and nr_of_minibatches % 30 == 0:\n",
    "              print (\"Cost after minibatch %i: %f\" % (nr_of_minibatches, cost_of_model))\n",
    "          if print_cost == True and nr_of_minibatches % 5 == 0:\n",
    "              costs.append(cost_of_model)  \n",
    "      except tf.errors.OutOfRangeError:\n",
    "        print('Done training, epoch reached')\n",
    "      finally:\n",
    "        coord.request_stop()\n",
    "        print(\"bitti\")\n",
    "        coord.join(threads)\n",
    "    \n",
    "      # plot the cost\n",
    "      plt.plot(np.squeeze(costs))\n",
    "      plt.ylabel('cost')\n",
    "      plt.xlabel('iterations (per tens)')\n",
    "      plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "      plt.show()\n",
    "    \n",
    "      # lets save the parameters in a variable\n",
    "      parameters = sess.run(parameters)\n",
    "\n",
    "    with tf.Session() as sess2:\n",
    "    \n",
    "      sess2.run(init_global_var)\n",
    "      # initializing local variables needed to be able to set num_epochs\n",
    "      sess2.run(init_local_var)\n",
    "    \n",
    "      coord2 = tf.train.Coordinator()\n",
    "      threads2 = tf.train.start_queue_runners(sess=sess2, coord=coord2)\n",
    "      correct_prediction = []\n",
    "      try:\n",
    "        i = 1\n",
    "        num_epochs = 1\n",
    "        while not coord2.should_stop():\n",
    "        \n",
    "          # Do NOT shuffle the training data when calculating accuracy\n",
    "          X_e_batch, Y_e_batch = sess2.run([X_eval_batch, Y_eval_batch])\n",
    "          print(\"i:\" + str(i))\n",
    "          print(X_e_batch)\n",
    "          print(Y_e_batch)\n",
    "          i += 1\n",
    "          # Calculate the correct predictions\n",
    "          correct_prediction.append(tf.equal(tf.argmax(sess2.run(ZL, feed_dict={X_mini_batch: X_e_batch}), axis=0),\n",
    "                                             tf.argmax(tf.transpose(Y_e_batch), axis=0)))\n",
    "          \n",
    "      except tf.errors.OutOfRangeError:\n",
    "        print('Calculation of Accuracy completed')\n",
    "      finally:\n",
    "        coord2.request_stop()\n",
    "        coord2.join(threads2)\n",
    "      \n",
    "      #flatten the correct_prediction matrix so that tf.reduce_mean function can use it to calculate accuracy\n",
    "      correct_prediction = tf.reshape(correct_prediction, [-1])\n",
    "      train_accuracy = tf.reduce_mean(sess2.run(tf.cast(correct_prediction, dtype=tf.float32)))\n",
    "      # Calculate accuracy on the test set\n",
    "      print (\"Train Accuracy:\", sess2.run(train_accuracy))\n",
    "        \n",
    "    return parameters, train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after minibatch 30: 0.012748\n",
      "Cost after minibatch 60: 0.009848\n",
      "Done training, epoch reached\n",
      "bitti\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdd6adbbda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:1\n",
      "[[-1.  2.  3.  8.  5.]\n",
      " [ 1.  5.  8.  3.  3.]\n",
      " [ 9.  8.  7.  4.  2.]\n",
      " [ 4.  7.  2.  3.  8.]]\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "i:2\n",
      "[[3. 5. 6. 9. 4.]\n",
      " [4. 6. 3. 4. 3.]\n",
      " [3. 2. 8. 9. 1.]\n",
      " [5. 5. 5. 5. 5.]]\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "i:3\n",
      "[[ 1.  5.  4.  2.  7.]\n",
      " [-4.  2.  9.  4.  5.]\n",
      " [ 7.  7.  7.  7.  7.]\n",
      " [ 9.  9.  9.  9.  9.]]\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "i:4\n",
      "[[ 6.  3.  7.  8.  4.]\n",
      " [11. 11. 11. 11. 11.]\n",
      " [ 2.  2.  2.  2.  2.]\n",
      " [13. 13. 13. 13. 13.]]\n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "i:5\n",
      "[[ 6.  6.  6.  6.  6.]\n",
      " [10. 10. 10. 10. 10.]\n",
      " [ 3.  3.  3.  3.  3.]\n",
      " [14. 14. 14. 14. 14.]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "i:6\n",
      "[[ 4.  4.  4.  4.  4.]\n",
      " [12. 12. 12. 12. 12.]\n",
      " [ 8.  8.  8.  8.  8.]\n",
      " [ 1.  1.  1.  1.  1.]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "Calculation of Accuracy completed\n",
      "Train Accuracy: 0.5833333\n"
     ]
    }
   ],
   "source": [
    "# X/Y_train and X/Y_test need to be replaced with file paths\n",
    "X_train = np.array([[-1.,4.,-7.],[2.,6.,2.],[3.,3.,9.],[8.,4.,4.],[5.,3.,5.]])\n",
    "Y_train = np.array([[1., 1., 1.], [0., 0., 0.]])\n",
    "X_test = np.array([[2.,3.],[3.,4.],[1.,2.],[4.,5.],[7.,8.]])\n",
    "Y_test = np.array([[1.],[0.]])\n",
    "learning_rate = 0.01 \n",
    "num_epochs = 10\n",
    "minibatch_size = 4     # m % minibatch_size needs to be 0. otherwise, tf.reshape function crashing \n",
    "num_units_in_layers = [5,4,3,2] \n",
    "lambd = 0. \n",
    "print_cost = True\n",
    "\n",
    "# Data may reside in several files. 2 is created here for illustration purposes\n",
    "train_path1 = \"train1.csv\"\n",
    "train_path2 = \"train2.csv\"\n",
    "train_input_paths = [train_path1, train_path2]\n",
    "\n",
    "parameters, train_accuracy = nn_model(train_input_paths, X_test, Y_test, learning_rate, num_epochs,\n",
    "                                      minibatch_size, num_units_in_layers, lambd, print_cost)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 5]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "#TODO\n",
    "# Implement using CSV file(s) even for test data\n",
    "# Split data into train, dev, test sets (maybe by using tf.Split(...) function) while still supporting csv files\n",
    "# Save NN parameters somewhere after (How?)\n",
    "# Add batch normalization   (check what needs to be done for gradient calculations)\n",
    "# Dropout\n",
    "\n",
    "abc = []\n",
    "abc.append(1)\n",
    "abc.append(5)\n",
    "print(abc)\n",
    "print(sum(abc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "der = np.array([3,3,3])\n",
    "print(len(der))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
