{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTION: create_placeholders\n",
    "# Example call:   X, Y = create_placeholders(100, 6)\n",
    "def create_nn_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "   \n",
    "    Arguments:\n",
    "    n_x -- scalar, size of the input X \n",
    "    n_y -- scalar, number of classes in Y \n",
    "   \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n",
    "   \n",
    "    Note:\n",
    "    - None is used as the second dimension below because it enables flexibility on the number of examples\n",
    "      we may have in the placeholders.\n",
    "      Note that the number of examples during test/train is different.\n",
    "    \"\"\"\n",
    "\n",
    "    X = tf.placeholder(shape=[n_x, None], dtype=tf.float32)\n",
    "    Y = tf.placeholder(shape=[n_y, None], dtype=tf.float32)\n",
    "   \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: \n",
    "# Example call:   tf_variable = get_nn_parameter(...)\n",
    "def get_nn_parameter(variable_scope, variable_name, dim1, dim2):\n",
    "    '''\n",
    "    Used to retrieve or create new NN parameters (weights & biases)\n",
    "    When calling, the corresponding NNparameter's dimensions need to be specified too.\n",
    "    Returns a tensorflow variable. Note that NN parameters need to be tensorflow variables\n",
    "    so that values can be changed whenever needed when training. Also note that it is \n",
    "    explicitly defined that the created variable is TRAINABLE.\n",
    "    '''\n",
    "    with tf.variable_scope(variable_scope, reuse=tf.AUTO_REUSE):\n",
    "      v = tf.get_variable(variable_name, \n",
    "                          [dim1, dim2], \n",
    "                          trainable=True, \n",
    "                          initializer = tf.contrib.layers.xavier_initializer())\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: \n",
    "# Example call:   nn_parameters = initialize_layer_parameters([3, 5, 5, 2])\n",
    "def initialize_layer_parameters(num_units_in_layers):\n",
    "    '''\n",
    "    NOTE THAT THE LAST LAYER HAS TO HAVE AT LEAST 2 UNITS BCZ SOFTMAX IS USED IN THIS NN MODEL\n",
    "    Returns a dictionary of created weights and biases for all layers of the NN.\n",
    "    Note that # units can vary in each layer.\n",
    "    Exmaple return: parameters = {\"W1\": tf_variable_for_W1, \"b1\": tf_variable_for_b1, ...}\n",
    "    '''\n",
    "    \n",
    "    parameters = {}\n",
    "    L = len(num_units_in_layers)\n",
    "     \n",
    "    for i in range (1, L):\n",
    "        #print(\"W\" + str(i) + \" \" + str(num_units_in_layers[i]) + \" \" + str(num_units_in_layers[i-1]))\n",
    "        temp_weight = get_nn_parameter(\"weights\",\n",
    "                                       \"W\"+str(i), \n",
    "                                       num_units_in_layers[i], \n",
    "                                       num_units_in_layers[i-1])\n",
    "        parameters.update({\"W\" + str(i) : temp_weight})  \n",
    "        \n",
    "        #print(\"b\" + str(i) + \" \" + str(num_units_in_layers[i]) + \" \" + str(1))\n",
    "        temp_bias = get_nn_parameter(\"biases\",\n",
    "                                     \"b\"+str(i), \n",
    "                                     num_units_in_layers[i], \n",
    "                                     1)\n",
    "        parameters.update({\"b\" + str(i) : temp_bias})  \n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: \n",
    "# Example call: ZL = forward_propagation_with_relu(X_train, [3, 5, 5, 2], nn_parameters)\n",
    "def forward_propagation_with_relu(X, num_units_in_layers, parameters):\n",
    "    '''\n",
    "    Returns ZL, which is the linear output of the output layer (layer L)\n",
    "    AL is also calculated but it is not returned or used. The optimizer function takes ZL as input, not the AL.\n",
    "    What activation function is used in output layer plays role when calculating the cost \n",
    "    so that you need to call the right cost (or cross entropy) function in compute_cost(...) function\n",
    "    '''\n",
    "    L = len(num_units_in_layers)\n",
    "    \n",
    "    A_temp = tf.transpose(X)\n",
    "    for i in range (1, L):\n",
    "        #W = get_nn_parameter(\"weights\", \"W\"+str(i), num_units_in_layers[i], num_units_in_layers[i-1])\n",
    "        W = parameters.get(\"W\"+str(i))\n",
    "        #b = get_nn_parameter(\"biases\", \"b\"+str(i), num_units_in_layers[i], 1)\n",
    "        b = parameters.get(\"b\"+str(i))\n",
    "        Z_temp = tf.add(tf.matmul(W, A_temp), b)\n",
    "        A_temp = tf.nn.relu(Z_temp)       # Example: A1 = relu(Z1)1, Note that A in the last (output) \n",
    "                                          # layer is irrelevant bcz we return ZL\n",
    "\n",
    "    return Z_temp   #This is the linear output of last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION\n",
    "# This function requires update if softmax is not used in the output layer\n",
    "def compute_cost(ZL, Y, parameters, lambd, mb_size):\n",
    "    \"\"\"\n",
    "    This function should be used for multinomial mutually exclusive classification, i.e. pick one out of N classes. \n",
    "    Also applicable when N = 2.\n",
    "    The labels must be one-hot encoded or can contain soft class probabilities: a particular example can belong to\n",
    "    class A with 50% probability and class B with 50% probability. Note that strictly speaking it doesn't mean that\n",
    "    it belongs to both classes, but one can interpret the probabilities this way.\n",
    "    \n",
    "    Arguments:\n",
    "    ZL -- output of forward propagation (output of the last LINEAR unit)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as ZL\n",
    "    mb_size -- number of samples in the given mini-batch\n",
    "    lambd -- lambda regularization parameter (regularization deactivated is lambd=0.)\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(ZL)\n",
    "    labels = Y\n",
    "    \n",
    "    # This cost calculation is unregularized. cost = (1/m) sum(Loss(y_hat(i), y(i))), where i = 1,..,mb_size \n",
    "    #tf.reduce_mean(..) function finds the mean of costs of examples in the given mini-batch\n",
    "    cost_unregularized = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = labels))\n",
    "    \n",
    "    # Add L2 regularization: cost += (lambd / (2 * mb_size)) * sum(W(i,j)**2), where i:1,..,n[l] and j:1,..,n[l-1] \n",
    "    # L:number of layers. Since the dict parameters includes both W and b, it needs to be divided with 2 to find L\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    # The list will have L elements, each holding the sum of weight matrix values in each layer. Later, these\n",
    "    # weight values need to be summed up again\n",
    "    list_sum_weights = []\n",
    "    \n",
    "    for i in range (0, L):\n",
    "        list_sum_weights.append(tf.nn.l2_loss(parameters.get(\"W\"+str(i+1))))\n",
    "    \n",
    "    # in the following calculation, since the l2_loss returns \"sum(t ** 2) / 2\", where the sum of squares is already\n",
    "    # divided by 2, there is no need to bultiply the mb_size with 2\n",
    "    #regularization_effect = (lambd / mb_size) * sum(list_sum_weights)\n",
    "    regularization_effect = tf.multiply((lambd / mb_size), tf.add_n(list_sum_weights))\n",
    "    cost = tf.add(cost_unregularized, regularization_effect)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that 4 statements below need to updated prior to any use in any project because number of columns \n",
    "# (# features and labels) may change in each project\n",
    "\n",
    "# This function only uses tf.TextLineReader(...) and does not read other types of files i.e. binary files\n",
    "def get_next_mini_batch(mini_batch_size, num_epochs, input_paths, shuffle):\n",
    "\n",
    "    # string_input_producer creates a FIFO queue for holding the filenames until the reader needs them\n",
    "    filename_queue = tf.train.string_input_producer(input_paths,\n",
    "                                                    num_epochs,\n",
    "                                                    shuffle)\n",
    "\n",
    "    # Select the type of reader that will be used to read the CSV files down below.\n",
    "    reader = tf.TextLineReader(skip_header_lines=1)\n",
    "\n",
    "    # reader.read(..) just reads 1 row at a time\n",
    "    key, value = reader.read(filename_queue)\n",
    "\n",
    "    # UPDATE-1 IN EACH PROJECT (depending on default values for each column)\n",
    "    # Determine default values for each column in case data is missing\n",
    "    record_defaults = [[\"\"], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]\n",
    "\n",
    "    # UPDATE-2 IN EACH PROJECT (depending on #columns)\n",
    "    # The decode_csv op parses the result of reader.read_up_to(...) into a list of tensors.\n",
    "    # For instance, col2 below is a list of tensors.\n",
    "    col1, col2, col3, col4, col5, col6, col7, col8 = tf.decode_csv(value, record_defaults)\n",
    "\n",
    "    # UPDATE-3-4 (FINAL) IN EACH PROJECT (depending on # features & labels)\n",
    "    # define which columns constitute features and which columns are labels and stack them together\n",
    "    features = tf.stack([col2, col3, col4, col5, col6])\n",
    "    labels = tf.stack([col7, col8])\n",
    "\n",
    "\n",
    "    min_after_dequeue = mini_batch_size * 3\n",
    "    capacity = min_after_dequeue + 10 * mini_batch_size\n",
    "\n",
    "    X_mini_batch, Y_mini_batch = tf.train.shuffle_batch([features, labels], \n",
    "                                                        batch_size=mini_batch_size, \n",
    "                                                        capacity=capacity,\n",
    "                                                        min_after_dequeue = min_after_dequeue,\n",
    "                                                        allow_smaller_final_batch = True)\n",
    "    \n",
    "    return X_mini_batch, Y_mini_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(train_input_paths, X_test, Y_test, learning_rate, num_epochs,\n",
    "             minibatch_size, num_units_in_layers, lambd, print_cost):\n",
    "    \"\"\"\n",
    "    Returns NN parameters after the completion of training.\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.reset_default_graph()     # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)       # tf.reset_default_graph() needs to be run first before calling tf.set_random_seed(..)\n",
    "    \n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # To be used to get the next mini batch during the training\n",
    "    X_mini_batch, Y_mini_batch = get_next_mini_batch(minibatch_size, \n",
    "                                                     num_epochs, \n",
    "                                                     train_input_paths, \n",
    "                                                     True)\n",
    "    \n",
    "    # To be used to get the next mini batch when calculating/evaluating accuracy\n",
    "    X_eval_batch, Y_eval_batch = get_next_mini_batch(minibatch_size, \n",
    "                                                     1,                     # Only run through dataset once\n",
    "                                                     train_input_paths, \n",
    "                                                     False)                 # Do NOT shuffle when calculating accuracy\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_layer_parameters(num_units_in_layers)\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    ZL = forward_propagation_with_relu(X_mini_batch, num_units_in_layers, parameters)\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    cost = compute_cost(ZL, Y_mini_batch, parameters, lambd, \n",
    "                        mb_size=tf.cast(X_mini_batch.shape[1], dtype=tf.float32))\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    optimizer =  tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init_global_var = tf.global_variables_initializer() \n",
    "    init_local_var = tf.local_variables_initializer()   \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "      sess.run(init_global_var)\n",
    "      # initializing local variables needed to be able to set num_epochs\n",
    "      sess.run(init_local_var)\n",
    "\n",
    "      # Start populating the filename queue.\n",
    "      # tf.train.start_queue_runners(...) needs to be called before populating the queue ...\n",
    "      # before you call run or eval to execute the read\n",
    "      coord = tf.train.Coordinator()\n",
    "      threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "      cost_of_model = 0\n",
    "      nr_of_minibatches = 0\n",
    "      try:\n",
    "        while not coord.should_stop():\n",
    "          _ , minibatch_cost = sess.run([optimizer, cost])\n",
    "            \n",
    "          nr_of_minibatches += 1\n",
    "          cost_of_model += minibatch_cost\n",
    "          cost_of_model /= nr_of_minibatches\n",
    "        \n",
    "          # Print the cost every epoch\n",
    "          if print_cost == True and nr_of_minibatches % 30 == 0:\n",
    "              print (\"Cost after minibatch %i: %f\" % (nr_of_minibatches, cost_of_model))\n",
    "          if print_cost == True and nr_of_minibatches % 5 == 0:\n",
    "              costs.append(cost_of_model)  \n",
    "      except tf.errors.OutOfRangeError:\n",
    "        print('Done training, epoch reached')\n",
    "      finally:\n",
    "        coord.request_stop()\n",
    "        print(\"bitti\")\n",
    "        coord.join(threads)\n",
    "    \n",
    "      # plot the cost\n",
    "      plt.plot(np.squeeze(costs))\n",
    "      plt.ylabel('cost')\n",
    "      plt.xlabel('iterations (per tens)')\n",
    "      plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "      plt.show()\n",
    "    \n",
    "      # lets save the parameters in a variable\n",
    "      parameters = sess.run(parameters)\n",
    "\n",
    "    with tf.Session() as sess2:\n",
    "    \n",
    "      sess2.run(init_global_var)\n",
    "      # initializing local variables needed to be able to set num_epochs\n",
    "      sess2.run(init_local_var)\n",
    "    \n",
    "      coord2 = tf.train.Coordinator()\n",
    "      threads2 = tf.train.start_queue_runners(sess=sess2, coord=coord2)\n",
    "      correct_prediction = []\n",
    "      try:\n",
    "        i = 1\n",
    "        num_epochs = 1\n",
    "        while not coord2.should_stop():\n",
    "        \n",
    "          # Do NOT shuffle the training data when calculating accuracy\n",
    "          X_e_batch, Y_e_batch = sess2.run([X_eval_batch, Y_eval_batch])\n",
    "          print(\"i:\" + str(i))\n",
    "          print(X_e_batch)\n",
    "          print(Y_e_batch)\n",
    "          i += 1\n",
    "          # Calculate the correct predictions\n",
    "          correct_prediction.append(tf.equal(tf.argmax(sess2.run(ZL, feed_dict={X_mini_batch: X_e_batch}), axis=0),\n",
    "                                             tf.argmax(tf.transpose(Y_e_batch), axis=0)))\n",
    "          \n",
    "      except tf.errors.OutOfRangeError:\n",
    "        print('Calculation of Accuracy completed')\n",
    "      finally:\n",
    "        coord2.request_stop()\n",
    "        coord2.join(threads2)\n",
    "      \n",
    "      #flatten the correct_prediction matrix so that tf.reduce_mean function can use it to calculate accuracy\n",
    "      correct_prediction = tf.reshape(correct_prediction, [-1])\n",
    "      accuracy = tf.reduce_mean(sess2.run(tf.cast(correct_prediction, dtype=tf.float32)))\n",
    "      # Calculate accuracy on the test set\n",
    "      print (\"Train Accuracy:\", sess2.run(accuracy))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after minibatch 30: 0.024913\n",
      "Cost after minibatch 60: 0.014315\n",
      "Done training, epoch reached\n",
      "bitti\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3Xl8XHW9//HXJ3smTZdJ0z1JF7pQBIqkRS5aKyCLIuXnBQFBQVHAK9fr9vCi3p968eoPRe9Vrwsgq4os4lax2gKyCAhtWAqUUlq6pAtp06ZNuiTN9vn9cU7aaZpl2mZyJpn38/GYR2bOnDP5TJd5zznfzdwdERGRnmRFXYCIiKQ/hYWIiPRKYSEiIr1SWIiISK8UFiIi0iuFhYiI9EphIRnNzP5iZldEXYdIulNYSCTMbJ2ZnRl1He5+rrvfHXUdAGb2uJl9oh9+T76Z3WFmDWZWY2af72X/z4X71YfH5Sc8900ze8XMWs3sG6muXaKjsJBBy8xyoq6hQzrVAnwDmApUAO8BvmRm53S1o5mdDVwPnAFMBCYD/5mwy2rgS8CfU1eupAOFhaQdMzvPzF4ys51m9oyZnZDw3PVm9qaZ7TKz18zs/yQ8d6WZPW1m/2NmdcA3wm1Pmdn3zGyHma01s3MTjtn/bT6JfSeZ2ZPh737EzH5iZr/q5j3MM7ONZvbvZlYD3GlmI8zsITOrDV//ITObEO7/LeBdwI/NbLeZ/TjcPsPMHjazOjNbaWYf6oM/4o8C33T3He6+Avg5cGU3+14B3O7uy919B/DNxH3d/W53/wuwqw/qkjSmsJC0YmZvB+4ArgFKgFuABQmXPt4k+FAdRvAN91dmNjbhJU4B1gCjgG8lbFsJjAS+C9xuZtZNCT3t+2tgSVjXN4CP9PJ2xgBxgm/wVxP8f7szfFwONAI/BnD3rwJ/B65z9yHufp2ZFQEPh793FHAp8FMzO66rX2ZmPw0Dtqvby+E+I4BxwLKEQ5cBXb5muL3zvqPNrKSX9y6DjMJC0s0ngVvc/Tl3bwvbE/YB7wBw99+4+2Z3b3f3+4FVwJyE4ze7+/+6e6u7N4bb1rv7z929DbgbGAuM7ub3d7mvmZUDs4GvuXuzuz8FLOjlvbQDX3f3fe7e6O7b3f237r7X3XcRhNm7ezj+PGCdu98Zvp8XgN8CF3a1s7v/i7sP7+bWcXY2JPxZn3BoPVDcTQ1DutiXHvaXQUphIemmAvhC4rdioIzg2zBm9tGES1Q7gbcRnAV02NDFa9Z03HH3veHdIV3s19O+44C6hG3d/a5Ete7e1PHAzGJmdouZrTezBuBJYLiZZXdzfAVwSqc/i8sIzliO1O7w59CEbUPp/jLS7i72pYf9ZZBSWEi62QB8q9O34pi732tmFQTX168DStx9OPAqkHhJKVXTKL8FxM0slrCtrJdjOtfyBWA6cIq7DwXmhtutm/03AE90+rMY4u6f6uqXmdnNYXtHV7flAGG7w1vAiQmHnggs7+Y9LO9i3y3uvr37ty2DkcJCopRrZgUJtxyCMLjWzE6xQJGZvd/MioEigg/UWgAz+xjBmUXKuft6oIqg0TzPzE4FPnCYL1NM0E6x08ziwNc7Pb+FoLdRh4eAaWb2ETPLDW+zzezYbmq8NgyTrm6JbRK/AP4jbHCfQXDp765uav4FcJWZzQzbO/4jcd+wpgKCz5Kc8O+xuzMlGcAUFhKlhQQfnh23b7h7FcGH14+BHQRdM68EcPfXgO8D/yD4YD0eeLof670MOBXYDvwXcD9Be0qyfgAUAtuAZ4G/dnr+h8CFYU+pH4XtGmcBlwCbCS6RfQfI5+h8naCjwHrgCeAmd/8rgJmVh2ci5QDh9u8Cj4X7r+fgkPs5wd/dpcBXw/u9NfzLAGRa/EjkyJjZ/cDr7t75DEFk0NGZhUiSwktAU8wsy4JBbPOBP0Rdl0h/SGlYmNk54UCi1WZ2fRfPf96CgVUvm9mjYQNmx3NtYa+Xl8ysty6KIv1hDPA4QQ+hHwGfcvcXI61IpJ+k7DJU2Mj1BvBeYCOwFLg0vO7csc97gOfcfa+ZfQqY5+4Xh8/tdvfuujeKiEg/SuWZxRxgtbuvcfdm4D6C0/b93P2xhH7rzwITUliPiIgcoVRObjaegwctbSSYSqE7VwF/SXhcYGZVQCtwo7sfcm3YzK4mmEaBoqKik2fMmHHURYuIZJLnn39+m7uX9rZfKsOiq7l3urzmZWaXA5UcPPVBubtvNrPJwN/M7BV3f/OgF3O/FbgVoLKy0quqqvqmchGRDGFm65PZL5WXoTZy8AjXCQR9xQ9iwZoGXwXOd/f9fdbdfXP4cw1Bo+JJKaxVRER6kMqwWApMtWBa5zyCgUUH9Woys5MIZhU93923Jmwf0THLqJmNBE4DXkNERCKRsstQ7t5qZtcBi4Bs4A53X25mNwBV7r4AuIlgkrbfhLNAV7v7+cCxwC1m1k4QaDcm9qISEZH+NWhGcKvNQkTk8JnZ8+5e2dt+GsEtIiK9UliIiEivFBYiItKrjA+L+sYWfvDIGyzbsDPqUkRE0lbGh4UZ/OCRVTy7Rgt/iYh0J+PDYmhBLiNiuayv29v7ziIiGSrjwwKgvKSI6u0KCxGR7igsgIp4jPV1e6IuQ0QkbSksgIqSGJt3NtHS1h51KSIiaUlhAZTHY7S1O5t2NEZdiohIWlJYABUlRQBq5BYR6YbCguAyFED1drVbiIh0RWEBjCrOpyA3i/XqESUi0iWFBWBmlMdjugwlItINhUWoPK6xFiIi3VFYhCpKYlTX7WWwrO8hItKXFBahipIYjS1t1O7a1/vOIiIZRmERKo8HPaLUbiEiciiFRWj/WAu1W4iIHEJhERo/vJAs01gLEZGuKCxCeTlZjBteqMtQIiJdUFgkqCiJ6TKUiEgXFBYJyuNFVOvMQkTkEAqLBBUlMer2NLOrqSXqUkRE0orCIkFFR/dZXYoSETmIwiJBWRgWuhQlInIwhUWCjqnKdWYhInIwhUWC4oJc4kV5VGs9bhGRgygsOimPq/usiEhnCotONNZCRORQCotOKuIx3qpvpLm1PepSRETShsKik/KSItodNu1sjLoUEZG0obDo5ECPKDVyi4h0SGlYmNk5ZrbSzFab2fVdPP95M3vNzF42s0fNrCLhuSvMbFV4uyKVdSaq0FgLEZFDpCwszCwb+AlwLjATuNTMZnba7UWg0t1PAB4EvhseGwe+DpwCzAG+bmYjUlVrotLifApzs9XILSKSIJVnFnOA1e6+xt2bgfuA+Yk7uPtj7t7xqfwsMCG8fzbwsLvXufsO4GHgnBTWup+ZqfusiEgnqQyL8cCGhMcbw23duQr4yxEe26fKS2IamCcikiCVYWFdbPMudzS7HKgEbjqcY83sajOrMrOq2traIy60s4p4jOq6vbh3Wa6ISMZJZVhsBMoSHk8ANnfeyczOBL4KnO/u+w7nWHe/1d0r3b2ytLS0zwqvKInR1NLO1l37et9ZRCQDpDIslgJTzWySmeUBlwALEncws5OAWwiCYmvCU4uAs8xsRNiwfVa4rV+UlxQBmlBQRKRDysLC3VuB6wg+5FcAD7j7cjO7wczOD3e7CRgC/MbMXjKzBeGxdcA3CQJnKXBDuK1fHFjXQu0WIiIAOal8cXdfCCzstO1rCffP7OHYO4A7Uldd98aPKCQ7yzTWQkQkpBHcXcjNzmLc8AJdhhIRCSksulERL2K9zixERACFRbfKS2JUq81CRARQWHSrIh5jx94WGppaoi5FRCRyCotudMw+W612CxERhUV3yuMaayEi0kFh0Y3yjnUtNEeUiIjCojtD8nMoKcrTZSgRERQWPSov0VTlIiKgsOhRx+yzIiKZTmHRg/KSIjbXN7KvtS3qUkREIqWw6EFFPIY7bNzRGHUpIiKRUlj0QGMtREQCCose7O8+q2k/RCTDKSx6UDokn1hetiYUFJGMp7DogZlRHo/pMpSIZDyFRS/K1X1WRERh0ZuKkiAs2ts96lJERCKjsOhFeUkR+1rb2bprX9SliIhERmHRi4q4ekSJiCgselGxf/ZZtVuISOZSWPRi3PBCsrNMPaJEJKMpLHqRm53F+OGFOrMQkYymsEhCRUmMarVZiEgGU1gkoTwe05mFiGQ0hUUSKkpi7NzbQn1jS9SliIhEQmGRhPJ4EaDZZ0UkcyksknCg+6zaLUQkMyksklC+f2CezixEJDMpLJJQlJ/DyCF5ugwlIhlLYZGkoEeULkOJSGZSWCSpoqRIZxYikrEUFkkqj8d4q6GJfa1tUZciItLvFBZJqiiJ4Q4b6hqjLkVEpN+lNCzM7BwzW2lmq83s+i6en2tmL5hZq5ld2Om5NjN7KbwtSGWdyejoPlutdgsRyUA5qXphM8sGfgK8F9gILDWzBe7+WsJu1cCVwBe7eIlGd5+VqvoOV8fAPHWfFZFMlLKwAOYAq919DYCZ3QfMB/aHhbuvC59rT2EdfWLkkDxiedkKCxHJSKm8DDUe2JDweGO4LVkFZlZlZs+a2QVd7WBmV4f7VNXW1h5Nrb0yM8rjwXrcIiKZJpVhYV1s88M4vtzdK4EPAz8wsymHvJj7re5e6e6VpaWlR1pn0ipKYlpeVUQyUirDYiNQlvB4ArA52YPdfXP4cw3wOHBSXxZ3JCpKitiwo5H29sPJPBGRgS+VYbEUmGpmk8wsD7gESKpXk5mNMLP88P5I4DQS2jqiUh6P0dzaTk1DU9SliIj0q5SFhbu3AtcBi4AVwAPuvtzMbjCz8wHMbLaZbQQuAm4xs+Xh4ccCVWa2DHgMuLFTL6pI7J99Vo3cIpJhUtkbCndfCCzstO1rCfeXElye6nzcM8DxqaztSFR0rGtRt4dTp5REXI2ISP/RCO7DMG54ATlZph5RIpJxFBaHISc7i/EjCnUZSkQyjsLiMGmshYhkIoXFYQrGWigsRCSzKCwOU0W8iPrGFur3tkRdiohIv1FYHKbyju6zmn1WRDKIwuIwaayFiGQihcVhKo93rGuhsBCRzKGwOEyxvBxGDsnXhIIiklEUFkdAPaJEJNMkFRZmdlEy2zJFhcZaiEiGSfbM4stJbssI5SUxahqaaGppi7oUEZF+0eNEgmZ2LvA+YLyZ/SjhqaFAayoLS2cVJTHcYeOOvRwzqjjqckREUq63M4vNQBXQBDyfcFsAnJ3a0tJXeTj7rNotRCRT9Hhm4e7LgGVm9mt3b4FgYSKgzN139EeB6UhjLUQk0yTbZvGwmQ01sziwDLjTzP47hXWltZKiPIrystXILSIZI9mwGObuDcAHgTvd/WTgzNSVld7MjPKSIo21EJGMkWxY5JjZWOBDwEMprGfAqIjHWK8zCxHJEMmGxQ0Ea2m/6e5LzWwysCp1ZaW/ipIYG+saaWv3qEsREUm5pNbgdvffAL9JeLwG+OdUFTUQlJfEaG5rp6ahifHDC6MuR0QkpZIdwT3BzH5vZlvNbIuZ/dbMJqS6uHRWsb/7rNotRGTwS/Yy1J0EYyvGAeOBP4XbMlZH99lqdZ8VkQyQbFiUuvud7t4a3u4CSlNYV9obO6yAnCxTI7eIZIRkw2KbmV1uZtnh7XJgeyoLS3c52VlMGFGoMwsRyQjJhsXHCbrN1gBvARcCH0tVUQNFeUmRllcVkYyQbFh8E7jC3UvdfRRBeHwjZVUNEBXxYF0Ld3WfFZHBLdmwOCFxLih3rwNOSk1JA0dFSYxdTa3UN7ZEXYqISEolGxZZ4QSCAIRzRCU1RmMw61iPWxMKishgl+wH/veBZ8zsQcAJ2i++lbKqBoiKknCsRd1eTiwbHnE1IiKpk+wI7l+YWRVwOmDAB939tZRWNgB0nFlUa2CeiAxySV9KCsMh4wMiUWFeNqOK83UZSkQGvWTbLKQb5Zp9VkQygMLiKJWXxDQwT0QGvZSGhZmdY2YrzWy1mV3fxfNzzewFM2s1sws7PXeFma0Kb1ekss6jUREvoqahiaaWtqhLERFJmZSFhZllAz8BzgVmApea2cxOu1UDVwK/7nRsHPg6cAowB/h6YtfddNIxoeAGXYoSkUEslWcWc4DV7r7G3ZuB+4D5iTu4+zp3fxlo73Ts2cDD7l4XDgZ8GDgnhbUesfISjbUQkcEvlWExHtiQ8HhjuK3PjjWzq82sysyqamtrj7jQo1HRMTBPZxYiMoilMiysi23JTqKU1LHufqu7V7p7ZWlpNDOmx4vyGJKfo7EWIjKopTIsNgJlCY8nAJv74dh+ZWbqPisig14qw2IpMNXMJplZHnAJwWp7yVgEnGVmI8KG7bPCbWmpQt1nRWSQS1lYuHsrcB3Bh/wK4AF3X25mN5jZ+QBmNtvMNgIXAbeY2fLw2DqCadGXhrcbwm1pqbwkxoYde2lr11TlIjI4pXTmWHdfCCzstO1rCfeXElxi6urYO4A7UllfX6mIF9HS5rxV38iEEbGoyxER6XMawd0HOsZa6FKUiAxWCos+UK7usyIyyCks+sC44YXkZpsG5onIoKWw6APZWcaEETGq6zTWQkQGJ4VFHymPx3RmISKDlsKij3SMtXBX91kRGXwUFn2kPB5j175WduxtiboUEZE+p7DoIxUlRQCs1xxRIjIIKSz6yP6xFuo+KyKDkMKij+wfa6FGbhEZhBQWfaQgN5vRQ/MVFiIyKCks+lB5PKblVUVkUFJY9KHyeBHrNTBPRAYhhUUfqiiJsaVhH00tbVGXIiLSpxQWfUg9okRksFJY9CH1iBKRwUph0Yc0ME9EBiuFRR8aEculOD9Hl6FEZNBRWPQhM6O8RLPPisjgo7DoYxUlMZ1ZiMigo7DoY+XxIjbu2Etbu6YqF5HBIyfqAgabipIYLW3O5p2NlIW9o45UU0sbq7bs5vWaBgpysznvhLGYWR9VKiKSPIVFH6uIHxhrkWxYtLU71XV7WVnTwOs1u1gZ3tZt30PiCcrKml188ezpqShbRKRHCos+Vl5yYKzFaccc+vy23ftYWbMrDIUGVtbs4o0tu2kMR32bBYEzfUwx5504jhljipk+ppjb/r6GHz+2msK8bD79ni5eWEQkhRQWfWzssEJys403tuxi2YadB4JhSxAM23Y379+3pCiP6WOKuXRO+f5QmDp6CLG8Q/9a/uuC49nb3MZNi1ZSlJfNladN6s+3JSIZTmHRx7KzjLIRMe56Zh13PbMOgILcLKaNLuY900cxfUwxM8YMZfqYYkqL8w/rdb930Yk0NrfxjT+9Riwvhw/NLkvRuxAROZjCIgW++v5jeWVTfXi2MJTyeIzsrKNvmM7NzuJ/P3wSn7i7in//3csU5GVz/onj+qBiEZGemfvg6OJZWVnpVVVVUZfRLxqb27jijiW8UL2Dn11+Mu+dOTrqkkRkgDKz5929srf9NM5iACrMy+b2Kys5btxQPn3PCzy1alvUJYnIIKewGKCKC3K5++NzmFxaxCd/UcXSdXVRlyQig5jCYgAbHsvjl1edwthhBXz8zqW8vHFn1CWJyCClsBjgSovzueeTpzAslstH71jCyppdUZckIoOQwmIQGDuskHs+cQr5OVlcdttzrN2m9TREpG+lNCzM7BwzW2lmq83s+i6ezzez+8PnnzOzieH2iWbWaGYvhbebU1nnYFBRUsQ9nziFdncu+/mzbNyhmW9FpO+kLCzMLBv4CXAuMBO41MxmdtrtKmCHux8D/A/wnYTn3nT3WeHt2lTVOZgcM6qYX141h937WrnstufY2tAUdUkiMkik8sxiDrDa3de4ezNwHzC/0z7zgbvD+w8CZ5imVT0qx40bxl0fn0Ptrn1cdttz1O1p7v0gEZFepDIsxgMbEh5vDLd1uY+7twL1QEn43CQze9HMnjCzd3X1C8zsajOrMrOq2travq1+AHt7+Qhuv2I21XV7+egdz9HQ1BJ1SSIywKUyLLo6Q+g8XLy7fd4Cyt39JODzwK/NbOghO7rf6u6V7l5ZWlp61AUPJqdOKeHmy09mZc0uPnbnUvY2t0ZdkogMYKkMi41A4kx3E4DN3e1jZjnAMKDO3fe5+3YAd38eeBOYlsJaB6X3zBjFjy45iRerd/DJX1TRFE6DLiJyuFIZFkuBqWY2yczygEuABZ32WQBcEd6/EPibu7uZlYYN5JjZZGAqsCaFtQ5a5x4/lpsuPJGnV2/n0/e8QEtbe9QlicgAlLKwCNsgrgMWASuAB9x9uZndYGbnh7vdDpSY2WqCy00d3WvnAi+b2TKChu9r3V3zWRyhfz55At+84G08+vpWPnv/S1ofXEQOW0qnKHf3hcDCTtu+lnC/Cbioi+N+C/w2lbVlmo+8o4LG5la+vfB1CnOz+e4/n0BWH0ybLiKZQetZZJCr505hz742fvjoKmJ52fzn+cehnsoikgyFRYb57JlT2dvcys//vpY3a3fzrQuOZ+LIoqjLEpE0p7mhMoyZ8ZX3Hcs3L3gbL2+o5+wfPMlPHluthm8R6ZHCIgOZGR95RwWPfOHdnD5jFDctWsl5P3qK59fviLo0EUlTCosMNnpoAT+7/GRu+2glu5pauPDmZ/i/f3hVI75F5BAKC+HMmaNZ/Pl3c+U/TeSe59Zz5vef4C+vvMVgWZ9dRI6ewkIAGJKfw9c/cBx/+PRpjBySz6fueYFP/uJ5Nu9sjLo0EUkDCgs5yAkThrPgutP4yvtm8PTqbbz3v5/gjqfWaiCfSIZTWMghcrKzuHruFBZ/bi6VE+Pc8NBrfPCnT7N8c33UpYlIRBQW0q2yeIy7PjabH116Ept2NnL+j5/m2wtXaAZbkQyksJAemRnnnziORz8/jw9VTuDWJ9dw1v88yeMrt0Zdmoj0I4WFJGVYLJf/98ETeOCaU8nPyeLKO5fyr/e+SO2ufVGXJiL9QGEhh2XOpDgL/+1dfPbMqSx6tYYzvv849y2ppl0N4CKDmg2WvvSVlZVeVVUVdRkZZfXW3Xzl96+wZG0dcybGuahyAsMKcxlamMvQglyGFuYwtDCXIXk5muFWJE2Z2fPuXtnrfgoLORrt7c5vnt/Atxe+Tn1j1yO/swyKO8KjIDcIlITHQwvDbQmPhxbkMmZYAcMKc/v5HYlklmTDQrPOylHJyjIunl3O/Fnjqd21j/rGFhoaW2hoaqGhsZWGppaEba37n1uzbff+5/c2d7/c6/TRxcyeNILZE+PMmRRn7LDCfnx3ItJBYSF9oiA3m7J47KBF15PV0tbOrqbWQ4JmTe1ulqyr4/cvbOJXz1YDMGFEIXMmxpk9Kc7siXGmlBZpTQ6RfqCwkMjlZmcRL8ojXpTX5fOtbe2seGsXS9bVsXRtHU+8UcvvXtwEQElRHpUTD5x5zBw7lJxs9dsQ6Wtqs5ABx91Zs20PS9fWBQGyro4NdcEcVkV52by9YsT+s49ZZcMpyM2OuOLeNbW08adlm8nPzeb9x48lWx0CpJ+ogVsySk190/4zj6Xr6li5ZRfukJttnDBhOLMnxnnH5DjvPGZkWp151De28Ktn13Pn0+vYtjsYszJjTDFfed+xzJ1WGnF1kgkUFpLR6ve2ULW+bn+AvLKpnpY2Z1RxPhfPLuPi2WVMGBGLrL6a+iZuf2oNv36umj3NbcydVso1cyezY28z3/nr62yoa2TutFK+fO4Mjh07NLI6ZfBTWIgkaGxu46nV27h3STWPhVOVzJtWyqVzyjl9xqh+O9tYvXUXNz+xhj++tIm2due8E8Zxzbsnc9y4Yfv32dfaxi//sZ7//dtqGppauOjkCXz+vdMZM6ygX2qUA9Zu28PTq7dx9nFjKC3Oj7qclFBYiHRj085G7l9Szf1VG9jSsI8xQwv40OwyLpldxrjhqema+/z6On72+BoeWbGFgtwsLq4s4xPvmkxZvPuzm517m/nx31bzi3+sJysLrn7XZK5+9xSG5KtfSqq9uqmenz3+JgtffQt3KMzN5uPvnMjVc6cMurE/CguRXrS2tfPo61u5d0k1T7xRiwHvmT6KD59Szrzpo466kbm93fnb61u5+Yk3qVq/g+GxXD566kSuOLWCkiHJf0ut3r6X7y56nYdefouRQ/L53HuncnFlWVq1vRypuj3NFOZmU5gXfScEd+fZNXX89PHV/H3VNorzc/jIqRWcOXM0dz69jj8t28zQghyunTeFj/3TpLSouS8oLEQOw4a6vdy/dAP3V22gdtc+xg4r2N+2cbgDAZtb21mwbDO3PPEmq7buZvzwQj7xrklcPLuMWN6RnxW8WL2Dby9cwdJ1Ozhm1BC+fO4MTp8xasCNM1m7bQ+LltewaHkNL1bvJJaXzdnHjWH+rHGRdEBob3ceWbGFnz7+Ji9t2MnIIflc9c5JXPaOcoYWHDiLWL65nu8vfoO/vb6V0uJ8PnP6MVw8u5y8nIEd2goLkSPQ0tbOoyu2cM9z1fx91TayDE6fMZrLTiln7rTSHs82du9r5b4l1dz+1Freqm9ixphirnn3ZM47YRy5ffQB6O4sfm0LN/7lddZu28M7Jsf56vtmcvyEYb0fHBF359VNDSxaXsPi12p4Y8tuAI4fP4wzjx1NTUMjf375LRqaWhk5JI/zThjH/FnjmFU2PKVB2NLWzoKXNnNzGOpl8UKumTuFC0+e0GN366Xr6rjprytZsq6OsnghnztzGvNnjR+w3Z0VFiJHqXr7Xu5dWs1vqjawbXcz44cX7j/bGD30QGNz7a593PXMWn75j/U0NLVyyqQ4186bwrxppSn7sGtpa+feJdX84JFV1O1p5oJZ4/ji2dMj7eGVqLWtnSXr6li8fAuLl9ewub6J7CxjzsQ4Zx03mrOOG8P4hPahfa1tPL6ylj++tIlHVmylubWdiSUxzp81ngtmjWNy6ZA+q62xuY37llZz29/XsmlnIzPGFPOpeVN4//Fjkz6rcXeeeKOWmxatZPnmBqaNHsIXzprOWTNHD7gzPYWFSB9pbm3nkRVb+PVz1Ty1ehvZWcYZM0ZxwUnjeXr1Nn7z/EZa2to5e+YYrnn3ZE4qH9FvtTU0tXDz429y+1NrceBjp03kX+YdE0kjbFNLG0++Ucvi17bw6Iot7NjbQn5OFu+aWsrZx43mzGNHM6KbUfqJGppa+OurNfzxpU088+Z23OGECcOYP2s8HzhxLKOKj6xXWP3eFu7+xzrpY6OiAAAMdklEQVTuemYddXuamT1xBP8y7xjmTT/yUG9vd/7yag3ff3gla2r3cGLZcL509nROO2bkEb1eFBQWIimwbtse7l1azYNVG9m+p5m87Cw++PbxfHLuZKb04bffw7V5ZyPfW7yS37+4ieGFuXzmjKlcdkpFyq+n1+9t4dHXt7B4+RaeeKOWxpY2hhbkcMaxozn7uNHMnVZ6VO00Wxqa+NOyzfz+xU0s39xAlsFpx4xk/qzxnPO2MUn1DOs8puWMGaO4dt4UZk+MH3FdnbW2tfO7Fzbxg0feYHN9E6cdU8IXz5rer18cjpTCQiSF9rW2sWRtHdNGFx90SSpqr26q59sLV/DMm9uZWBLjmndPYeSQfPJyssjLziI/N/yZkxVsC7cn3u/tW3ZNfROLX6th8fItPLtmO63tzuih+Zw1cwxnHzeGUybH+6yNJtHqrbv4w4ub+eOyTWyoa6QgN4szjx3NBbPGM3da6SHBuKZ2N7c+uYbfvbCJNnc+cMJYrp03hRljUjfIsamljV8/V81PHlvN9j3NnDVzNF84azrTxxSn7HceLYWFSIZydx5fWcu3F65g1dbdh318Xk4W+YkBkhAorW3Oyi27AJhcWsTZx43hrJmjOXHC8H5b4MrdeaF6B394cTMPvbyZHXtbGB7L5f3Hj+WCk8ZTkJPNzU8EYyTysrP4UGUZV8/teUxLX9u9r5U7n1rLrU+uYXdzKxfMGs/nzpxGeUl6tCklUliIZLi2dufN2t00tbTR3NpOc2s7+9ra999vbm2nOXy8r7X3fZpb22lzZ/bEOGcfN5pjRkX/bbmlrZ2/r6rlDy9uZvFrNTS1tAPsHyPxsdMmRTryeseeZm5+4k3uemYdbe3OJXPK+MzpUxnVy9mou9Pa7rS2Oa3t7bS2OS3hz/3b2p2WtmBbQW72EZ+9KCxEJKPs2dfK4tdq2NXUygUnjT9ojETUtjQ08aNHV3H/0g1kZxnjhhceCIGEQDjw8/A+l2eVDecPnz7tiGpTWIiIpJn12/dw+1Nr2bG3hZwsC27ZWeRmGzlZWeRkJ2wLfwaPu9mWFRw7PJbHyRVH1pieFsuqmtk5wA+BbOA2d7+x0/P5wC+Ak4HtwMXuvi587svAVUAb8Bl3X5TKWkVEUq2ipIgb5r8t6jKOSMr61ZlZNvAT4FxgJnCpmc3stNtVwA53Pwb4H+A74bEzgUuA44BzgJ+GryciIhFIZSfsOcBqd1/j7s3AfcD8TvvMB+4O7z8InGFBv735wH3uvs/d1wKrw9cTEZEIpDIsxgMbEh5vDLd1uY+7twL1QEmSx2JmV5tZlZlV1dbW9mHpIiKSKJVh0VWn686t6d3tk8yxuPut7l7p7pWlpVqCUkQkVVIZFhuBsoTHE4DN3e1jZjnAMKAuyWNFRKSfpDIslgJTzWySmeURNFgv6LTPAuCK8P6FwN886Mu7ALjEzPLNbBIwFViSwlpFRKQHKes66+6tZnYdsIig6+wd7r7czG4Aqtx9AXA78EszW01wRnFJeOxyM3sAeA1oBT7t7m2pqlVERHqmQXkiIhks40Zwm1ktsP4oXmIksK2PykmFdK8P0r/GdK8PVGNfSPf6IL1qrHD3XnsIDZqwOFpmVpVMukYl3euD9K8x3esD1dgX0r0+GBg1djawVxoXEZF+obAQEZFeKSwOuDXqAnqR7vVB+teY7vWBauwL6V4fDIwaD6I2CxER6ZXOLEREpFcKCxER6VXGh4WZnWNmK81stZldH3U9nZlZmZk9ZmYrzGy5mf1b1DV1xcyyzexFM3so6lq6YmbDzexBM3s9/LM8NeqaEpnZ58K/31fN7F4z63mR5v6p6Q4z22pmryZsi5vZw2a2Kvx5ZMuzpbbGm8K/55fN7PdmNjzdakx47otm5mY2MoraDkdGh0WSCzRFrRX4grsfC7wD+HQa1gjwb8CKqIvowQ+Bv7r7DOBE0qhWMxsPfAaodPe3EUyPc0m0VQFwF8HiY4muBx5196nAo+HjKN3FoTU+DLzN3U8A3gC+3N9FdXIXh9aImZUB7wWq+7ugI5HRYUFyCzRFyt3fcvcXwvu7CD7kDlnbI0pmNgF4P3Bb1LV0xcyGAnMJ5iLD3ZvdfWe0VR0iBygMZ1+OkQazLLv7kwRztiVKXLDsbuCCfi2qk65qdPfF4fo4AM8SzFodmW7+HCFYHfRLdLH8QjrK9LBIapGldGFmE4GTgOeireQQPyD4R98edSHdmAzUAneGl8puM7OiqIvq4O6bgO8RfMN8C6h398XRVtWt0e7+FgRfZIBREdfTm48Df4m6iM7M7Hxgk7svi7qWZGV6WCS1yFI6MLMhwG+Bz7p7Q9T1dDCz84Ct7v581LX0IAd4O/Azdz8J2EP0l0/2C6/7zwcmAeOAIjO7PNqqBj4z+yrBZdx7oq4lkZnFgK8CX4u6lsOR6WExIBZZMrNcgqC4x91/F3U9nZwGnG9m6wgu451uZr+KtqRDbAQ2unvHGdmDBOGRLs4E1rp7rbu3AL8D/inimrqzxczGAoQ/t0ZcT5fM7ArgPOAyT7/BZFMIvhgsC//fTABeMLMxkVbVi0wPi2QWaIqUmRnBtfYV7v7fUdfTmbt/2d0nuPtEgj+/v7l7Wn0rdvcaYIOZTQ83nUGwVkq6qAbeYWax8O/7DNKoAb6TxAXLrgD+GGEtXTKzc4B/B853971R19OZu7/i7qPcfWL4/2Yj8Pbw32nayuiwCBvBOhZoWgE84O7Lo63qEKcBHyH4xv5SeHtf1EUNQP8K3GNmLwOzgG9HXM9+4RnPg8ALwCsE/y8jnw7CzO4F/gFMN7ONZnYVcCPwXjNbRdCT58Y0rPHHQDHwcPj/5eY0rHHA0XQfIiLSq4w+sxARkeQoLEREpFcKCxER6ZXCQkREeqWwEBGRXiksJO2Z2TPhz4lm9uE+fu2vdPW7UsXMLjCzlIzc7fxe+ug1jzezu/r6dWXgUddZGTDMbB7wRXc/7zCOyXb3th6e3+3uQ/qiviTreYZgsNi2o3ydQ95Xqt6LmT0CfNzdB8TsqJIaOrOQtGdmu8O7NwLvCgdafS5cQ+MmM1sarl1wTbj/vHANkF8TDHLDzP5gZs+Ha0ZcHW67kWCm15fM7J7E32WBm8L1JV4xs4sTXvtxO7A2xj3hqGvM7EYzey2s5XtdvI9pwL6OoDCzu8zsZjP7u5m9Ec6z1bE2SFLvK+G1u3ovl5vZknDbLRZMyY+Z7Tazb5nZMjN71sxGh9svCt/vMjN7MuHl/0R6TJkuUXJ33XRL6xuwO/w5D3goYfvVwH+E9/OBKoI5d+YRTBY4KWHfePizEHgVKEl87S5+1z8TrIuQDYwmmJJjbPja9QTz+WQRjMx9JxAHVnLgbH14F+/jY8D3Ex7fBfw1fJ2pBNM+FBzO++qq9vD+sQQf8rnh458CHw3vO/CB8P53E37XK8D4zvUTzCLwp6j/HegW7S0n2VARSUNnASeY2YXh42EEH7rNwBJ3X5uw72fM7P+E98vC/bb38NrvBO714FLPFjN7ApgNNISvvRHAzF4CJhKsm9AE3GZmfwa6WjFwLMFU6YkecPd2YJWZrQFmHOb76s4ZwMnA0vDEp5ADk/41J9T3PMG0HQBPA3eZ2QMEkxl22EowG65kMIWFDGQG/Ku7LzpoY9C2safT4zOBU919r5k9TvANvrfX7s6+hPttQI67t5rZHIIP6UsI5hw7vdNxjQQf/Ik6Nxo6Sb6vXhhwt7t3tUpci7t3/N42ws8Bd7/WzE4hWMjqJTOb5e7bCf6sGpP8vTJIqc1CBpJdBBPEdVgEfMqCKdwxs2nW9aJGw4AdYVDMIFietkNLx/GdPAlcHLYflBKstLeku8IsWG9kmLsvBD5LMFlhZyuAYzptu8jMssxsCsEiTSsP4311lvheHgUuNLNR4WvEzayip4PNbIq7P+fuXwO2cWD6/mkEl+4kg+nMQgaSl4FWM1tGcL3/hwSXgF4IG5lr6XqZz78C11ow4+xKgktGHW4FXjazF9z9soTtvwdOBZYRfNv/krvXhGHTlWLgj2ZWQPCt/nNd7PMk8H0zs4Rv9iuBJwjaRa519yYzuy3J99XZQe/FzP4DWGxmWUAL8GlgfQ/H32RmU8P6Hw3fO8B7gD8n8ftlEFPXWZF+ZGY/JGgsfiQcv/CQuz8YcVndMrN8gjB7px9Y11oykC5DifSvbwOxqIs4DOXA9QoK0ZmFiIj0SmcWIiLSK4WFiIj0SmEhIiK9UliIiEivFBYiItKr/w8cpUZmgQhtvwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdd6b0beb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:1\n",
      "[[ 9.  8.  7.  4.  2.]\n",
      " [ 4.  6.  3.  4.  3.]\n",
      " [-1.  2.  3.  8.  5.]]\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "i:2\n",
      "[[14. 14. 14. 14. 14.]\n",
      " [10. 10. 10. 10. 10.]\n",
      " [11. 11. 11. 11. 11.]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "i:3\n",
      "[[-4.  2.  9.  4.  5.]\n",
      " [ 1.  5.  4.  2.  7.]\n",
      " [13. 13. 13. 13. 13.]]\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "i:4\n",
      "[[12. 12. 12. 12. 12.]\n",
      " [ 3.  2.  8.  9.  1.]\n",
      " [ 3.  5.  6.  9.  4.]]\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "i:5\n",
      "[[6. 3. 7. 8. 4.]\n",
      " [6. 6. 6. 6. 6.]\n",
      " [5. 5. 5. 5. 5.]]\n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "i:6\n",
      "[[8. 8. 8. 8. 8.]\n",
      " [3. 3. 3. 3. 3.]\n",
      " [4. 7. 2. 3. 8.]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "i:7\n",
      "[[2. 2. 2. 2. 2.]\n",
      " [9. 9. 9. 9. 9.]\n",
      " [1. 5. 8. 3. 3.]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "i:8\n",
      "[[7. 7. 7. 7. 7.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [4. 4. 4. 4. 4.]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "Calculation of Accuracy completed\n",
      "Train Accuracy: 0.5833333\n"
     ]
    }
   ],
   "source": [
    "# X/Y_train and X/Y_test need to be replaced with file paths\n",
    "X_train = np.array([[-1.,4.,-7.],[2.,6.,2.],[3.,3.,9.],[8.,4.,4.],[5.,3.,5.]])\n",
    "Y_train = np.array([[1., 1., 1.], [0., 0., 0.]])\n",
    "X_test = np.array([[2.,3.],[3.,4.],[1.,2.],[4.,5.],[7.,8.]])\n",
    "Y_test = np.array([[1.],[0.]])\n",
    "learning_rate = 0.01 \n",
    "num_epochs = 10\n",
    "minibatch_size = 3 \n",
    "num_units_in_layers = [5,4,3,2] \n",
    "lambd = 0. \n",
    "print_cost = True\n",
    "\n",
    "# Data may reside in several files. 2 is created here for illustration purposes\n",
    "train_path1 = \"train1.csv\"\n",
    "train_path2 = \"train2.csv\"\n",
    "train_input_paths = [train_path1, train_path2]\n",
    "\n",
    "parameters = nn_model(train_input_paths, X_test, Y_test, learning_rate, num_epochs,\n",
    "         minibatch_size, num_units_in_layers, lambd, print_cost)\n",
    "\n",
    "#minibatch_size = 3 \n",
    "#accuracy = evaluate_nn_model(train_input_paths, num_units_in_layers, parameters, minibatch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 5]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "#TODO\n",
    "# Implement using CSV file(s) even for test data\n",
    "# Split data into train, dev, test sets (maybe by using tf.Split(...) function) while still supporting csv files\n",
    "# Save NN parameters somewhere after (How?)\n",
    "# Add batch normalization   (check what needs to be done for gradient calculations)\n",
    "# Dropout\n",
    "\n",
    "abc = []\n",
    "abc.append(1)\n",
    "abc.append(5)\n",
    "print(abc)\n",
    "print(sum(abc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "der = np.array([3,3,3])\n",
    "print(len(der))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
