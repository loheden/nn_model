{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTION: create_placeholders\n",
    "# Example call:   X, Y = create_placeholders(100, 6)\n",
    "def create_nn_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "   \n",
    "    Arguments:\n",
    "    n_x -- scalar, size of the input X \n",
    "    n_y -- scalar, number of classes in Y \n",
    "   \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n",
    "   \n",
    "    Note:\n",
    "    - None is used as the second dimension below because it enables flexibility on the number of examples\n",
    "      we may have in the placeholders.\n",
    "      Note that the number of examples during test/train is different.\n",
    "    \"\"\"\n",
    "\n",
    "    X = tf.placeholder(shape=[n_x, None], dtype=tf.float32)\n",
    "    Y = tf.placeholder(shape=[n_y, None], dtype=tf.float32)\n",
    "   \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: \n",
    "# Example call:   tf_variable = get_nn_parameter(...)\n",
    "def get_nn_parameter(variable_scope, variable_name, dim1, dim2):\n",
    "    '''\n",
    "    Used to retrieve or create new NN parameters (weights & biases)\n",
    "    When calling, the corresponding NNparameter's dimensions need to be specified too.\n",
    "    Returns a tensorflow variable. Note that NN parameters need to be tensorflow variables\n",
    "    so that values can be changed whenever needed when training. Also note that it is \n",
    "    explicitly defined that the created variable is TRAINABLE.\n",
    "    '''\n",
    "    with tf.variable_scope(variable_scope, reuse=tf.AUTO_REUSE):\n",
    "      v = tf.get_variable(variable_name, \n",
    "                          [dim1, dim2], \n",
    "                          trainable=True, \n",
    "                          initializer = tf.contrib.layers.xavier_initializer())\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: \n",
    "# Example call:   nn_parameters = initialize_layer_parameters([3, 5, 5, 2])\n",
    "def initialize_layer_parameters(num_units_in_layers):\n",
    "    '''\n",
    "    NOTE THAT THE LAST LAYER HAS TO HAVE AT LEAST 2 UNITS BCZ SOFTMAX IS USED IN THIS NN MODEL\n",
    "    Returns a dictionary of created weights and biases for all layers of the NN.\n",
    "    Note that # units can vary in each layer.\n",
    "    Exmaple return: parameters = {\"W1\": tf_variable_for_W1, \"b1\": tf_variable_for_b1, ...}\n",
    "    '''\n",
    "    \n",
    "    parameters = {}\n",
    "    L = len(num_units_in_layers)\n",
    "     \n",
    "    for i in range (1, L):\n",
    "        #print(\"W\" + str(i) + \" \" + str(num_units_in_layers[i]) + \" \" + str(num_units_in_layers[i-1]))\n",
    "        temp_weight = get_nn_parameter(\"weights\",\n",
    "                                       \"W\"+str(i), \n",
    "                                       num_units_in_layers[i], \n",
    "                                       num_units_in_layers[i-1])\n",
    "        parameters.update({\"W\" + str(i) : temp_weight})  \n",
    "        \n",
    "        #print(\"b\" + str(i) + \" \" + str(num_units_in_layers[i]) + \" \" + str(1))\n",
    "        temp_bias = get_nn_parameter(\"biases\",\n",
    "                                     \"b\"+str(i), \n",
    "                                     num_units_in_layers[i], \n",
    "                                     1)\n",
    "        parameters.update({\"b\" + str(i) : temp_bias})  \n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: \n",
    "# Example call: ZL = forward_propagation_with_relu(X_train, [3, 5, 5, 2], nn_parameters)\n",
    "def forward_propagation_with_relu(X, num_units_in_layers, parameters):\n",
    "    '''\n",
    "    Returns ZL, which is the linear output of the output layer (layer L)\n",
    "    AL is also calculated but it is not returned or used. The optimizer function takes ZL as input, not the AL.\n",
    "    What activation function is used in output layer plays role when calculating the cost \n",
    "    so that you need to call the right cost (or cross entropy) function in compute_cost(...) function\n",
    "    '''\n",
    "    L = len(num_units_in_layers)\n",
    "    \n",
    "    A_temp = X\n",
    "    for i in range (1, L):\n",
    "        #W = get_nn_parameter(\"weights\", \"W\"+str(i), num_units_in_layers[i], num_units_in_layers[i-1])\n",
    "        W = parameters.get(\"W\"+str(i))\n",
    "        #b = get_nn_parameter(\"biases\", \"b\"+str(i), num_units_in_layers[i], 1)\n",
    "        b = parameters.get(\"b\"+str(i))\n",
    "        Z_temp = tf.add(tf.matmul(W, A_temp), b)\n",
    "        A_temp = tf.nn.relu(Z_temp)       # Example: A1 = relu(Z1)1, Note that A in the last (output) \n",
    "                                          # layer is irrelevant bcz we return ZL\n",
    "\n",
    "    return Z_temp   #This is the linear output of last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION\n",
    "# This function requires update if softmax is not used in the output layer\n",
    "def compute_cost(ZL, Y, parameters, mb_size, lambd):\n",
    "    \"\"\"\n",
    "    This function should be used for multinomial mutually exclusive classification, i.e. pick one out of N classes. \n",
    "    Also applicable when N = 2.\n",
    "    The labels must be one-hot encoded or can contain soft class probabilities: a particular example can belong to\n",
    "    class A with 50% probability and class B with 50% probability. Note that strictly speaking it doesn't mean that\n",
    "    it belongs to both classes, but one can interpret the probabilities this way.\n",
    "    \n",
    "    Arguments:\n",
    "    ZL -- output of forward propagation (output of the last LINEAR unit)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as ZL\n",
    "    mb_size -- number of samples in the given mini-batch\n",
    "    lambd -- lambda regularization parameter (regularization deactivated is lambd=0.)\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(ZL)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    \n",
    "    # This cost calculation is unregularized. cost = (1/m) sum(Loss(y_hat(i), y(i))), where i = 1,..,mb_size \n",
    "    #tf.reduce_mean(..) function finds the mean of costs of examples in the given mini-batch\n",
    "    cost_unregularized = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = labels))\n",
    "    \n",
    "    # Add L2 regularization: cost += (lambd / (2 * mb_size)) * sum(W(i,j)**2), where i:1,..,n[l] and j:1,..,n[l-1] \n",
    "    # L:number of layers. Since the dict parameters includes both W and b, it needs to be divided with 2 to find L\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    # The list will have L elements, each holding the sum of weight matrix values in each layer. Later, these\n",
    "    # weight values need to be summed up again\n",
    "    list_sum_weights = []\n",
    "    \n",
    "    for i in range (0, L):\n",
    "        list_sum_weights.append(tf.nn.l2_loss(parameters.get(\"W\"+str(i+1))))\n",
    "    \n",
    "    # in the following calculation, since the l2_loss returns \"sum(t ** 2) / 2\", where the sum of squares is already\n",
    "    # divided by 2, there is no need to bultiply the mb_size with 2\n",
    "    #regularization_effect = (lambd / mb_size) * sum(list_sum_weights)\n",
    "    regularization_effect = tf.multiply((lambd / mb_size), tf.add_n(list_sum_weights))\n",
    "    cost = tf.add(cost_unregularized, regularization_effect)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1387842\n"
     ]
    }
   ],
   "source": [
    "# TEST CODE FOR TESTING THE compute_cost(...) function\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(1)  # tf.reset_default_graph() needs to be run first before calling tf.set_random_seed(..)\n",
    "\n",
    "num_units_in_layers = [5,4,3,2]\n",
    "\n",
    "X = tf.placeholder(shape=[5, 3], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[2, 3], dtype=tf.float32)\n",
    "parameters = initialize_layer_parameters(num_units_in_layers)\n",
    "ZL = forward_propagation_with_relu(X, num_units_in_layers, parameters)\n",
    "cost = compute_cost(ZL, Y, parameters, 3, 0.05)\n",
    "optimizer =  tf.train.AdamOptimizer(learning_rate = 0.001).minimize(cost)\n",
    "init = tf.global_variables_initializer() \n",
    "\n",
    "test_sess = tf.Session()\n",
    "test_sess.run(init)\n",
    "_ , minibatch_cost = test_sess.run([optimizer, cost], \n",
    "                                   feed_dict={X: [[-1.,4.,-7.],[2.,6.,2.],[3.,3.,9.],[8.,4.,4.],[5.,3.,5.]], \n",
    "                                              Y: [[0.6, 0., 0.3], [0.4, 0., 0.7]]})\n",
    "\n",
    "#initial_cost = test_sess.run(cost, feed_dict={X: [[-1.,4.,-7.],[2.,6.,2.],[3.,3.,9.],[8.,4.,4.],[5.,3.,5.]],\n",
    "#                                              Y: [[0.6, 0., 0.3], [0.4, 0., 0.7]]})\n",
    "\n",
    "\n",
    "print(minibatch_cost)\n",
    "\n",
    "test_sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X_train, Y_train, X_test, Y_test, learning_rate, num_epochs,\n",
    "             minibatch_size, num_units_in_layers, lambd, print_cost):\n",
    "    \"\"\"\n",
    "    Returns NN parameters after the completion of training.\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.reset_default_graph()     # to be able to rerun the model without overwriting tf variables\n",
    "    #tf.set_random_seed(1)       # tf.reset_default_graph() needs to be run first before calling tf.set_random_seed(..)\n",
    "\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]                            # n_y : output size\n",
    "    X, Y = create_nn_placeholders(n_x, n_y)\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_layer_parameters(num_units_in_layers)\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    ZL = forward_propagation_with_relu(X, num_units_in_layers, parameters)\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    mb_size = tf.placeholder(tf.float32, name='mb_size')  #minibatch_size to use in regularization\n",
    "    cost = compute_cost(ZL, Y, parameters, mb_size, lambd)\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    optimizer =  tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer() \n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Do the training loop\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "        num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "        #seed = seed + 1\n",
    "        minibatches = num_minibatches\n",
    "            \n",
    "        for minibatch in range (minibatches):\n",
    "            #print(learning_rate)\n",
    "            # Select a minibatch\n",
    "            minibatch_X = X_train\n",
    "            minibatch_Y = Y_train\n",
    "                \n",
    "            # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "            # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "            numExamplesInCurrentBatch = minibatch_X.shape[1]\n",
    "            _ , minibatch_cost = sess.run([optimizer, cost], \n",
    "                                          feed_dict={X: minibatch_X, \n",
    "                                                     Y: minibatch_Y,\n",
    "                                                     mb_size: numExamplesInCurrentBatch})\n",
    "                \n",
    "            #print(\"numExamplesInCurrentBatch = \" + str(numExamplesInCurrentBatch))\n",
    "               \n",
    "            # IN CASE, YOU IMPLEMENT  DROPOUT REGULARIZATION, HERE IS THE EXAMPLE CODE. PLACE IT WHERE IT NEEDS TO BE\n",
    "            # IT JUST NEEDS TO BE USED THE COMPUTE_COST FUNCTION ACTUALLY...\n",
    "            # A1 = tf.nn.dropout(A1, keep_prob=keep_p)\n",
    "                \n",
    "            epoch_cost += minibatch_cost / num_minibatches\n",
    "            \n",
    "        # Print the cost every epoch\n",
    "        if print_cost == True and epoch % 10 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "        if print_cost == True and epoch % 5 == 0:\n",
    "            costs.append(epoch_cost)\n",
    "                \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    # lets save the parameters in a variable\n",
    "    parameters = sess.run(parameters)\n",
    "    print (\"Parameters have been trained!\")\n",
    "\n",
    "    # Calculate the correct predictions\n",
    "    correct_prediction = tf.equal(tf.argmax(ZL), tf.argmax(Y))\n",
    "\n",
    "    # Calculate accuracy on the test set\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "    print (\"Train Accuracy:\", accuracy.eval(session=sess, feed_dict={X: X_train, Y: Y_train}))\n",
    "    print (\"Test Accuracy:\", accuracy.eval(session=sess, feed_dict={X: X_test, Y: Y_test}))\n",
    "    \n",
    "    sess.close()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 1.211954\n",
      "Cost after epoch 10: 0.998918\n",
      "Cost after epoch 20: 0.817819\n",
      "Cost after epoch 30: 0.660246\n",
      "Cost after epoch 40: 0.136063\n",
      "Cost after epoch 50: 0.032631\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb512fa1470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters have been trained!\n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'W1': array([[-0.11512583,  0.19306064, -0.81568444, -0.051974  , -0.09166741],\n",
       "        [-0.820789  , -0.20797144,  0.09524529,  0.00412735,  0.73404497],\n",
       "        [-0.35475236, -1.0209918 ,  0.5200488 , -0.17201208,  0.33751267],\n",
       "        [-0.4849321 , -0.6735957 , -0.5696551 , -0.575947  , -0.8014692 ]],\n",
       "       dtype=float32),\n",
       " 'W2': array([[ 0.07387549, -0.8189524 , -0.78105015, -0.4279087 ],\n",
       "        [-0.31153423,  0.5844176 , -0.03496258,  0.06460911],\n",
       "        [ 0.32245827,  0.82985896, -0.02295909, -0.4270891 ]],\n",
       "       dtype=float32),\n",
       " 'W3': array([[ 0.05739209,  0.5837955 ,  0.8621709 ],\n",
       "        [-0.6989815 , -0.2702042 , -0.18813634]], dtype=float32),\n",
       " 'b1': array([[ 0.9870131 ],\n",
       "        [-0.62879145],\n",
       "        [ 0.2566456 ],\n",
       "        [ 0.28253913]], dtype=float32),\n",
       " 'b2': array([[0.623573 ],\n",
       "        [1.083892 ],\n",
       "        [1.3184326]], dtype=float32),\n",
       " 'b3': array([[1.0303432],\n",
       "        [0.9391197]], dtype=float32)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X/Y_train and X/Y_test need to be replaced with file paths\n",
    "X_train = np.array([[-1.,4.,-7.],[2.,6.,2.],[3.,3.,9.],[8.,4.,4.],[5.,3.,5.]])\n",
    "Y_train = np.array([[1., 1., 1.], [0., 0., 0.]])\n",
    "X_test = np.array([[2.],[3.],[1.],[4.],[7.]])\n",
    "Y_test = np.array([[1.],[0.]])\n",
    "learning_rate = 0.01 \n",
    "num_epochs = 60\n",
    "minibatch_size = 3 \n",
    "num_units_in_layers = [5,4,3,2] \n",
    "lambd = 0. \n",
    "print_cost = True\n",
    "\n",
    "nn_model(X_train, Y_train, X_test, Y_test, learning_rate, num_epochs,\n",
    "         minibatch_size, num_units_in_layers, lambd, print_cost)\n",
    "                  \n",
    "#parameters = nn_model(X_train = np.array([[-1.,4.,-7.],[2.,6.,2.],[3.,3.,9.],[8.,4.,4.],[5.,3.,5.]]),\n",
    "#                      Y_train = np.array([[1., 1., 1.], [0., 0., 0.]]), \n",
    "#                      X_test = np.array([[2.],[3.],[1.],[4.],[7.]]),\n",
    "#                      Y_test = np.array([[1.],[0.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 5]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "#TODO\n",
    "# Add tf.Dataset functionality and read data from csv files\n",
    "# Split data into train, dev, test sets by using tf.Split(...) function\n",
    "# Add mini-batch support with shuffling implemented   (use tf.train.shuffle_batch(...))\n",
    "# Save NN parameters somewhere after (How?)\n",
    "\n",
    "abc = []\n",
    "abc.append(1)\n",
    "abc.append(5)\n",
    "print(abc)\n",
    "print(sum(abc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "der = np.array([2,2,2])\n",
    "print(der.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
