{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTION: create_placeholders\n",
    "# Example call:   X, Y = create_placeholders(100, 6)\n",
    "def create_nn_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "   \n",
    "    Arguments:\n",
    "    n_x -- scalar, size of the input X \n",
    "    n_y -- scalar, number of classes in Y \n",
    "   \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n",
    "   \n",
    "    Note:\n",
    "    - None is used as the second dimension below because it enables flexibility on the number of examples\n",
    "      we may have in the placeholders.\n",
    "      Note that the number of examples during test/train is different.\n",
    "    \"\"\"\n",
    "\n",
    "    X = tf.placeholder(shape=[n_x, None], dtype=tf.float32)\n",
    "    Y = tf.placeholder(shape=[n_y, None], dtype=tf.float32)\n",
    "   \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: \n",
    "# Example call:   tf_variable = get_nn_parameter(...)\n",
    "def get_nn_parameter(variable_scope, variable_name, dim1, dim2):\n",
    "    '''\n",
    "    Used to retrieve or create new NN parameters (weights & biases)\n",
    "    When calling, the corresponding NNparameter's dimensions need to be specified too.\n",
    "    Returns a tensorflow variable. Note that NN parameters need to be tensorflow variables\n",
    "    so that values can be changed whenever needed when training. Also note that it is \n",
    "    explicitly defined that the created variable is TRAINABLE.\n",
    "    '''\n",
    "    with tf.variable_scope(variable_scope, reuse=tf.AUTO_REUSE):\n",
    "      v = tf.get_variable(variable_name, \n",
    "                          [dim1, dim2], \n",
    "                          trainable=True, \n",
    "                          initializer = tf.contrib.layers.xavier_initializer())\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: \n",
    "# Example call:   nn_parameters = initialize_layer_parameters([3, 5, 5, 2])\n",
    "def initialize_layer_parameters(num_units_in_layers):\n",
    "    '''\n",
    "    NOTE THAT THE LAST LAYER HAS TO HAVE AT LEAST 2 UNITS BCZ SOFTMAX IS USED IN THIS NN MODEL\n",
    "    Returns a dictionary of created weights and biases for all layers of the NN.\n",
    "    Note that # units can vary in each layer.\n",
    "    Exmaple return: parameters = {\"W1\": tf_variable_for_W1, \"b1\": tf_variable_for_b1, ...}\n",
    "    '''\n",
    "    \n",
    "    parameters = {}\n",
    "    L = len(num_units_in_layers)\n",
    "     \n",
    "    for i in range (1, L):\n",
    "        #print(\"W\" + str(i) + \" \" + str(num_units_in_layers[i]) + \" \" + str(num_units_in_layers[i-1]))\n",
    "        temp_weight = get_nn_parameter(\"weights\",\n",
    "                                       \"W\"+str(i), \n",
    "                                       num_units_in_layers[i], \n",
    "                                       num_units_in_layers[i-1])\n",
    "        parameters.update({\"W\" + str(i) : temp_weight})  \n",
    "        \n",
    "        #print(\"b\" + str(i) + \" \" + str(num_units_in_layers[i]) + \" \" + str(1))\n",
    "        temp_bias = get_nn_parameter(\"biases\",\n",
    "                                     \"b\"+str(i), \n",
    "                                     num_units_in_layers[i], \n",
    "                                     1)\n",
    "        parameters.update({\"b\" + str(i) : temp_bias})  \n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: \n",
    "# Example call: ZL = forward_propagation_with_relu(X_train, [3, 5, 5, 2], nn_parameters)\n",
    "def forward_propagation_with_relu(X, num_units_in_layers, parameters):\n",
    "    '''\n",
    "    Returns ZL, which is the linear output of the output layer (layer L)\n",
    "    AL is also calculated but it is not returned or used. The optimizer function takes ZL as input, not the AL.\n",
    "    What activation function is used in output layer plays role when calculating the cost \n",
    "    so that you need to call the right cost (or cross entropy) function in compute_cost(...) function\n",
    "    '''\n",
    "    L = len(num_units_in_layers)\n",
    "    \n",
    "    A_temp = X\n",
    "    for i in range (1, L):\n",
    "        #W = get_nn_parameter(\"weights\", \"W\"+str(i), num_units_in_layers[i], num_units_in_layers[i-1])\n",
    "        W = parameters.get(\"W\"+str(i))\n",
    "        #b = get_nn_parameter(\"biases\", \"b\"+str(i), num_units_in_layers[i], 1)\n",
    "        b = parameters.get(\"b\"+str(i))\n",
    "        print(\"W: \" + str(W.shape))\n",
    "        print(\"A_temp: \" + str(A_temp.shape))\n",
    "        print(\"b: \" + str(b.shape))\n",
    "        Z_temp = tf.add(tf.matmul(W, A_temp), b)\n",
    "        A_temp = tf.nn.relu(Z_temp)       # Example: A1 = relu(Z1)1, Note that A in the last (output) \n",
    "                                          # layer is irrelevant bcz we return ZL\n",
    "\n",
    "    return Z_temp   #This is the linear output of last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION\n",
    "# This function requires update if softmax is not used in the output layer\n",
    "def compute_cost(ZL, Y, parameters, mb_size, lambd):\n",
    "    \"\"\"\n",
    "    This function should be used for multinomial mutually exclusive classification, i.e. pick one out of N classes. \n",
    "    Also applicable when N = 2.\n",
    "    The labels must be one-hot encoded or can contain soft class probabilities: a particular example can belong to\n",
    "    class A with 50% probability and class B with 50% probability. Note that strictly speaking it doesn't mean that\n",
    "    it belongs to both classes, but one can interpret the probabilities this way.\n",
    "    \n",
    "    Arguments:\n",
    "    ZL -- output of forward propagation (output of the last LINEAR unit)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as ZL\n",
    "    mb_size -- number of samples in the given mini-batch\n",
    "    lambd -- lambda regularization parameter (regularization deactivated is lambd=0.)\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(ZL)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    \n",
    "    # This cost calculation is unregularized. cost = (1/m) sum(Loss(y_hat(i), y(i))), where i = 1,..,mb_size \n",
    "    #tf.reduce_mean(..) function finds the mean of costs of examples in the given mini-batch\n",
    "    cost_unregularized = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = labels))\n",
    "    \n",
    "    # Add L2 regularization: cost += (lambd / (2 * mb_size)) * sum(W(i,j)**2), where i:1,..,n[l] and j:1,..,n[l-1] \n",
    "    # L:number of layers. Since the dict parameters includes both W and b, it needs to be divided with 2 to find L\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    # The list will have L elements, each holding the sum of weight matrix values in each layer. Later, these\n",
    "    # weight values need to be summed up again\n",
    "    list_sum_weights = []\n",
    "    \n",
    "    for i in range (0, L):\n",
    "        list_sum_weights.append(tf.nn.l2_loss(parameters.get(\"W\"+str(i+1))))\n",
    "    \n",
    "    # in the following calculation, since the l2_loss returns \"sum(t ** 2) / 2\", where the sum of squares is already\n",
    "    # divided by 2, there is no need to bultiply the mb_size with 2\n",
    "    #regularization_effect = (lambd / mb_size) * sum(list_sum_weights)\n",
    "    regularization_effect = tf.multiply((lambd / mb_size), tf.add_n(list_sum_weights))\n",
    "    cost = tf.add(cost_unregularized, regularization_effect)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: (4, 5)\n",
      "A_temp: (5, 3)\n",
      "b: (4, 1)\n",
      "W: (3, 4)\n",
      "A_temp: (4, 3)\n",
      "b: (3, 1)\n",
      "W: (2, 3)\n",
      "A_temp: (3, 3)\n",
      "b: (2, 1)\n",
      "1.1387842\n"
     ]
    }
   ],
   "source": [
    "# TEST CODE FOR TESTING THE compute_cost(...) function\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(1)  # tf.reset_default_graph() needs to be run first before calling tf.set_random_seed(..)\n",
    "\n",
    "num_units_in_layers = [5,4,3,2]\n",
    "\n",
    "X = tf.placeholder(shape=[5, 3], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[2, 3], dtype=tf.float32)\n",
    "parameters = initialize_layer_parameters(num_units_in_layers)\n",
    "ZL = forward_propagation_with_relu(X, num_units_in_layers, parameters)\n",
    "cost = compute_cost(ZL, Y, parameters, 3, 0.05)\n",
    "optimizer =  tf.train.AdamOptimizer(learning_rate = 0.001).minimize(cost)\n",
    "init = tf.global_variables_initializer() \n",
    "\n",
    "test_sess = tf.Session()\n",
    "test_sess.run(init)\n",
    "_ , minibatch_cost = test_sess.run([optimizer, cost], \n",
    "                                   feed_dict={X: [[-1.,4.,-7.],[2.,6.,2.],[3.,3.,9.],[8.,4.,4.],[5.,3.,5.]], \n",
    "                                              Y: [[0.6, 0., 0.3], [0.4, 0., 0.7]]})\n",
    "\n",
    "#initial_cost = test_sess.run(cost, feed_dict={X: [[-1.,4.,-7.],[2.,6.,2.],[3.,3.,9.],[8.,4.,4.],[5.,3.,5.]],\n",
    "#                                              Y: [[0.6, 0., 0.3], [0.4, 0., 0.7]]})\n",
    "\n",
    "\n",
    "print(minibatch_cost)\n",
    "\n",
    "test_sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X_train, Y_train, X_test, Y_test, learning_rate, num_epochs,\n",
    "             minibatch_size, num_units_in_layers, lambd, print_cost):\n",
    "    \"\"\"\n",
    "    Returns NN parameters after the completion of training.\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.reset_default_graph()     # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)       # tf.reset_default_graph() needs to be run first before calling tf.set_random_seed(..)\n",
    "\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]                            # n_y : output size\n",
    "    X, Y = create_nn_placeholders(n_x, n_y)\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_layer_parameters(num_units_in_layers)\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    ZL = forward_propagation_with_relu(X, num_units_in_layers, parameters)\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    mb_size = tf.placeholder(tf.float32, name='mb_size')  #minibatch_size to use in regularization\n",
    "    cost = compute_cost(ZL, Y, parameters, mb_size, lambd)\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    optimizer =  tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer() \n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Do the training loop\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "        num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "        #seed = seed + 1\n",
    "        minibatches = num_minibatches\n",
    "            \n",
    "        for minibatch in range (minibatches):\n",
    "            #print(learning_rate)\n",
    "            # Select a minibatch\n",
    "            minibatch_X = X_train\n",
    "            minibatch_Y = Y_train\n",
    "                \n",
    "            # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "            # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "            numExamplesInCurrentBatch = minibatch_X.shape[1]\n",
    "            _ , minibatch_cost = sess.run([optimizer, cost], \n",
    "                                          feed_dict={X: minibatch_X, \n",
    "                                                     Y: minibatch_Y,\n",
    "                                                     mb_size: numExamplesInCurrentBatch})\n",
    "                \n",
    "            #print(\"numExamplesInCurrentBatch = \" + str(numExamplesInCurrentBatch))\n",
    "               \n",
    "            # IN CASE, YOU IMPLEMENT  DROPOUT REGULARIZATION, HERE IS THE EXAMPLE CODE. PLACE IT WHERE IT NEEDS TO BE\n",
    "            # IT JUST NEEDS TO BE USED THE COMPUTE_COST FUNCTION ACTUALLY...\n",
    "            # A1 = tf.nn.dropout(A1, keep_prob=keep_p)\n",
    "                \n",
    "            epoch_cost += minibatch_cost / num_minibatches\n",
    "            \n",
    "        # Print the cost every epoch\n",
    "        if print_cost == True and epoch % 10 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "        if print_cost == True and epoch % 5 == 0:\n",
    "            costs.append(epoch_cost)\n",
    "                \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    # lets save the parameters in a variable\n",
    "    parameters = sess.run(parameters)\n",
    "    print (\"Parameters have been trained!\")\n",
    "\n",
    "    # Calculate the correct predictions\n",
    "    correct_prediction = tf.equal(tf.argmax(ZL, axis=0), tf.argmax(Y, axis=0))\n",
    "\n",
    "    # Calculate accuracy on the test set\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "    print (\"Train Accuracy:\", accuracy.eval(session=sess, feed_dict={X: X_train, Y: Y_train}))\n",
    "    print (\"Test Accuracy:\", accuracy.eval(session=sess, feed_dict={X: X_test, Y: Y_test}))\n",
    "    \n",
    "    sess.close()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: (4, 5)\n",
      "A_temp: (5, ?)\n",
      "b: (4, 1)\n",
      "W: (3, 4)\n",
      "A_temp: (4, ?)\n",
      "b: (3, 1)\n",
      "W: (2, 3)\n",
      "A_temp: (3, ?)\n",
      "b: (2, 1)\n",
      "Cost after epoch 0: 0.052406\n",
      "Cost after epoch 10: 0.002728\n",
      "Cost after epoch 20: 0.000387\n",
      "Cost after epoch 30: 0.000142\n",
      "Cost after epoch 40: 0.000087\n",
      "Cost after epoch 50: 0.000068\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1017b06c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters have been trained!\n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'W1': array([[-0.51629895, -0.7485086 , -0.20134105, -0.37897936,  0.49125004],\n",
       "        [ 0.28973234, -0.53518665,  0.2100072 ,  0.41112044,  0.44235188],\n",
       "        [ 0.46325162,  0.44848445,  0.52507544,  0.50134546,  0.6612028 ],\n",
       "        [-0.39544895,  0.08629039,  0.35854337, -0.42334738, -0.32174394]],\n",
       "       dtype=float32),\n",
       " 'W2': array([[ 0.6474998 ,  0.6659541 ,  0.7333255 ,  0.8247205 ],\n",
       "        [ 0.28520772,  0.4674578 , -0.44765174,  0.31514168],\n",
       "        [-0.5309416 ,  0.02135521, -0.6851548 ,  0.06344235]],\n",
       "       dtype=float32),\n",
       " 'W3': array([[ 0.1085325 , -0.43001   , -0.5332509 ],\n",
       "        [-0.666517  ,  0.37457833,  0.35882998]], dtype=float32),\n",
       " 'b1': array([[0.83939946],\n",
       "        [1.0266683 ],\n",
       "        [0.72672164],\n",
       "        [0.9551141 ]], dtype=float32),\n",
       " 'b2': array([[0.81575596],\n",
       "        [0.13801348],\n",
       "        [1.1796874 ]], dtype=float32),\n",
       " 'b3': array([[ 0.5899908 ],\n",
       "        [-0.25317705]], dtype=float32)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X/Y_train and X/Y_test need to be replaced with file paths\n",
    "X_train = np.array([[-1.,4.,-7.],[2.,6.,2.],[3.,3.,9.],[8.,4.,4.],[5.,3.,5.]])\n",
    "Y_train = np.array([[1., 1., 1.], [0., 0., 0.]])\n",
    "X_test = np.array([[2.,3.],[3.,4.],[1.,2.],[4.,5.],[7.,8.]])\n",
    "Y_test = np.array([[1.],[0.]])\n",
    "learning_rate = 0.01 \n",
    "num_epochs = 60\n",
    "minibatch_size = 3 \n",
    "num_units_in_layers = [5,4,3,2] \n",
    "lambd = 0. \n",
    "print_cost = True\n",
    "\n",
    "nn_model(X_train, Y_train, X_test, Y_test, learning_rate, num_epochs,\n",
    "         minibatch_size, num_units_in_layers, lambd, print_cost)\n",
    "                  \n",
    "#parameters = nn_model(X_train = np.array([[-1.,4.,-7.],[2.,6.,2.],[3.,3.,9.],[8.,4.,4.],[5.,3.,5.]]),\n",
    "#                      Y_train = np.array([[1., 1., 1.], [0., 0., 0.]]), \n",
    "#                      X_test = np.array([[2.],[3.],[1.],[4.],[7.]]),\n",
    "#                      Y_test = np.array([[1.],[0.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 5]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "#TODO\n",
    "# Add tf.Dataset functionality and read data from csv files\n",
    "# Split data into train, dev, test sets by using tf.Split(...) function\n",
    "# Add mini-batch support with shuffling implemented   (use tf.train.shuffle_batch(...))\n",
    "# Save NN parameters somewhere after (How?)\n",
    "# Add batch normalization\n",
    "\n",
    "abc = []\n",
    "abc.append(1)\n",
    "abc.append(5)\n",
    "print(abc)\n",
    "print(sum(abc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "der = np.array([2,2,2])\n",
    "print(der.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
